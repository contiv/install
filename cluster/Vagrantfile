# -*- mode: ruby -*-
# vi: set ft=ruby :
require 'rubygems'
require 'fileutils'
require 'yaml'

DEFAULT_K8S_VERSION = 'v1.6.4'.freeze

# Guest OS versions for testing
UBUNTU = 'ubuntu'.freeze
CENTOS = 'centos'.freeze
RHEL = 'rhel7'.freeze

HOST_SHARED_FOLDER = './export/'.freeze
GUEST_SHARED_FOLDER = '/shared'.freeze
# Different orchestration platforms we support
ORC_LEGACY_SWARM = 'legacy-swarm'.freeze
ORC_SWARM = 'swarm-mode'.freeze
ORC_KUBEADM = 'kubeadm'.freeze
ORC_KUBEADM_IDX = 2

# Global vars
token = 'd900e1.8a392798f13b33a4'
node_os = ENV['CONTIV_NODE_OS'] || CENTOS
k8s_ver = ENV['CONTIV_K8S_VERSION'] || DEFAULT_K8S_VERSION
orc_path = case k8s_ver
           when /^v1\.[45]\./ then 'k8s1.4/'
           when /^v1\.6\./    then 'k8s1.6/'
           else
             raise "unsupported k8s version: #{k8s_ver}"
           end

swarm_path = 'docker17/'
orchestrators = [ORC_LEGACY_SWARM, ORC_SWARM, ORC_KUBEADM]

# method to create an etc_hosts file based on the cluster info
def create_etc_hosts(node_names, node_ips, o)
  master_ip = node_ips[0]
  hosts = "127.0.0.1   localhost\n"
  hosts << "#{master_ip}   netmaster\n"

  node_names.zip(node_ips).each do |node, ip|
    hosts << "#{ip}   #{node} \n"
  end

  etc_file = (ENV['VAGRANT_CWD'] || '.') + '/.etc_hosts_' + o
  File.write(etc_file, hosts)
end

# method to create an cfg file based on the cluster info
# This cfg file is used for ansible installations
def create_cfg_info(node_ips, o)
  node_os = ENV['CONTIV_NODE_OS'] || CENTOS
  conn = {}
  node_ips.each_with_index do |node_ip, n|
    node = if n.zero?
             { 'role' => 'master' }
           else
             {}
           end
    def_ctrl_if = node_os == UBUNTU ? 'enp0s8' : 'eth1'
    def_data_if = node_os == UBUNTU ? 'enp0s9' : 'eth2'
    node['control'] = ENV['CONTIV_CONTROL_IF'] || def_ctrl_if
    node['data'] = ENV['CONTIV_DATA_IF'] || def_data_if
    conn[node_ip] = node
  end
  cfg_data = { 'CONNECTION_INFO' => conn }
  cfg_file = (ENV['VAGRANT_CWD'] || '.') + '/.cfg_' + o + '.yaml'
  File.write(cfg_file, cfg_data.to_yaml)
end

provision_node = <<SCRIPT
echo "export https_proxy='$2'" >> /etc/profile.d/envvar.sh
echo "export http_proxy='$1'" >> ~/.profile
echo "export https_proxy='$2'" >> ~/.profile
source /etc/profile.d/envvar.sh
SCRIPT

provision_net_ubuntu = <<SCRIPT
sudo yum install -y net-tools
cat <<EOF > /etc/hosts
$1
EOF
ifup eth1
SCRIPT

provision_net_centos = <<SCRIPT
cat <<EOF > /etc/hosts
$1
EOF
SCRIPT

provision_key = <<SCRIPT
sudo apt-get install python -y
mkdir -p /home/ubuntu/.ssh/
cat <<EOF >> /home/ubuntu/.ssh/authorized_keys
$1
EOF
SCRIPT

# Begin execution here
num_nodes = ENV['CONTIV_NODES'] ? ENV['CONTIV_NODES'].to_i : 2
num_workers = num_nodes - 1
base_ip = ENV['CONTIV_IP_PREFIX'] || '192.168.2.'
ip = 50

# This configuration creates the configuration for all the VMs
# So we have num_nodes * orchestrators.size VMs that will be listed in
# vagrant status etc. All the VMs are marked autostart: false, so
# they have to be started explicitly using vagrant up <vm name>
# node_names are like kubeadm-master, kubeadm-worker0 etc for each
# orchestrator.
node_names = []
node_ips = []
node_orcs = []
master_ips = []

orchestrators.each do |o|
  # Save the start location in IP array
  start = node_ips.length

  node_names.push(o + '-master')
  num_workers.times do |n|
    node_names.push("#{o}-worker#{n}")
  end
  master_ips.push("#{base_ip}#{ip}")
  num_nodes.times do
    node_ips.push("#{base_ip}#{ip}")
    node_orcs.push(o)
    ip += 1
  end

  create_etc_hosts(node_names[start, num_nodes], node_ips[start, num_nodes], o)
  create_cfg_info(node_ips[start, num_nodes], o)
end

# Check for plugin pre-reqs.
# We need vagrant registration plugin to register redhat boxes.
if node_os == RHEL
  unless Vagrant.has_plugin?('vagrant-registration')
    raise 'vagrant-registration plugin is not installed!'
  end
end

VAGRANTFILE_API_VERSION = '2'.freeze
Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vm.box_check_update = false
  config.vbguest.auto_update = false if Vagrant.has_plugin?('vagrant-vbguest')

  if node_os == RHEL
    config.registration.manager = 'subscription_manager'
    config.registration.username = ENV['CONTIV_RHEL_USER']
    config.registration.password = ENV['CONTIV_RHEL_PASSWD']
  end
  config.vm.provider 'virtualbox' do |v|
    v.linked_clone = true if Vagrant::VERSION >= '1.8'
  end

  (num_nodes * orchestrators.size).times do |n|
    node_name = node_names[n]
    node_addr = node_ips[n]
    node_orc = node_orcs[n]

    config.vm.define node_name, autostart: false do |c|
      if node_os == RHEL
        # Download rhel7.2 box from https://access.redhat.com/downloads/content/293/ver=2/rhel---7/2.0.0/x86_64/product-software
        # Add it as rhel7 vagrant box add rhel-cdk-kubernetes-7.2-29.x86_64.vagrant-virtualbox.box --name=rhel7
        # run this command once you done with adding vagrant box : vagrant plugin install vagrant-registration
        # You need to export following variable, in order to run on RHEL7
        # export CONTIV_RHEL_USER=<your developer account username of redhat site>
        # export CONTIV_RHEL_PASSWD=<your developer account password of redhat site>
        # export CONTIV_NODE_OS=rhel7
        c.vm.box = 'rhel7'
        config.ssh.insert_key = false
      elsif node_os == UBUNTU
        c.vm.box = 'ubuntu/xenial64'
      else
        if node_orc == ORC_SWARM
          config.vm.box = 'contiv/centos73'
          config.vm.box_version = '0.10.1'
        else
          config.vm.box = 'centos/7'
        end
        config.ssh.insert_key = false
      end

      c.vm.provision 'shell' do |s|
        s.inline = provision_node
        s.args = [ENV['http_proxy'] || '', ENV['https_proxy'] || '']
      end
      if node_orc == ORC_SWARM
        config.vm.synced_folder HOST_SHARED_FOLDER, GUEST_SHARED_FOLDER, create: true
      end
      c.vm.provision 'shell' do |s|
        s.inline = node_os == UBUNTU ? provision_net_centos : provision_net_ubuntu
        etc_file = (ENV['VAGRANT_CWD'] || '.') + '/.etc_hosts_' + node_orc
        hosts = File.read(etc_file)
        s.args = [hosts]
      end

      # configure ip address etc
      c.vm.hostname = node_name
      c.vm.network :private_network, ip: node_addr
      c.vm.network :private_network, ip: node_addr, virtualbox__intnet: 'true', auto_config: false

      c.vm.provider 'virtualbox' do |v|
        v.memory = node_orc == ORC_KUBEADM && master_ips.include?(node_addr) ? 2048 : 512
        # make all nics 'virtio' to take benefit of builtin vlan tag
        # support, which otherwise needs to be enabled in Intel drivers,
        # which are used by default by virtualbox
        v.customize ['modifyvm', :id, '--nictype1', 'virtio']
        v.customize ['modifyvm', :id, '--nictype2', 'virtio']
        v.customize ['modifyvm', :id, '--nictype3', 'virtio']
        v.customize ['modifyvm', :id, '--nicpromisc2', 'allow-all']
        v.customize ['modifyvm', :id, '--nicpromisc3', 'allow-all']
        v.customize ['modifyvm', :id, '--paravirtprovider', 'kvm']
      end # v

      if node_os == UBUNTU
        c.vm.provision 'shell' do |s|
          s.inline = provision_key
          key = File.read(ENV['CONTIV_SSH_CERT'] || ENV['HOME'] + '/.ssh/id_rsa.pub')
          s.args = [key]
        end
      end
      if node_orc != ORC_KUBEADM && node_os != UBUNTU
        c.vm.provision :shell, inline: 'yum install policycoreutils-python -y'
      end

      if node_orc == ORC_KUBEADM
        # kubeadm specific bootstrap tasks
        if node_os == RHEL
          c.vm.provision 'shell', inline: <<-EOS
            yum remove -y kubernetes-node-*
            yum remove -y kubernetes-client-*
            EOS
        elsif node_os == UBUNTU
          c.vm.provision :shell, path: orc_path + 'bootstrap_ubuntu.sh'
        else
          c.vm.provision :shell, path: orc_path + 'bootstrap_centos.sh'
        end
        if master_ips.include? node_addr
          # Install kubernetes on master
          c.vm.provision :shell, path: orc_path + 'k8smaster.sh', args: [token, node_addr, k8s_ver, 'vagrant']
        else
          # Install kubernetes on nodes
          c.vm.provision :shell, path: orc_path + 'k8sworker.sh', args: [token, master_ips[ORC_KUBEADM_IDX]]
        end # if
      elsif node_orc == ORC_SWARM
        # swarm specific bootstrap tasks
        if node_os == CENTOS
          c.vm.provision :shell, path: swarm_path + 'bootstrap_centos.sh'
          if master_ips.include? node_addr
            # Start swarm on master
            c.vm.provision :shell, path: swarm_path + 'master.sh', args: [node_addr, GUEST_SHARED_FOLDER]
          else
            # Run the generated swarm join command line from worker
            c.vm.provision :shell, path: HOST_SHARED_FOLDER + 'worker.sh'
          end
        else
          raise 'Swarm install is currently supported only on CentOS'
        end
      end
    end # c
  end # role
end # config
